[导言]

在科学研究越来越依靠标准化精密计算手段的今天，用智能体技术来自动化加速科研的潜力让人心潮澎湃。可今天人们使用Cursor, Codex之类的智能体进行科研或执行重要任务时，仍然主要拿它们作为辅助工具，每运行几步就需要等待人验证或提出修改意见。那么如果我们完全交由智能体自动化，如何让它足够可信地代替人完成一部分需要精密思考的科研工作呢？

这是一个宏大而开放的问题。我们近期完成的，倾注一年半心血的科研项目，就在一个具体的科学场景中，初步探索答案。我们着眼于通过基因表达分析这个任务，并通过它来挖掘控制各种混淆因子影响后的基因-表型关联。[简单概括一下这个任务是干什么的] 这一方面是因为它非常重要 (列举一下探索基因-表型关联有什么应用，比如xxx)；另一方面是因为它足够难（概括一下它为什么难）（它的难是否在现实的很多科研任务中有代表性）（对智能体技术有什么考验）

[为什么自动化转录组分析这么难]

在开始接到这个课题的很长一段时间里，我们是一筹莫展的。考虑混淆后的基因-表型关联绝大部分是未知的，无法在现有文献中验证。在研发这样的智能体之前，为了评估它的性能好坏，我们需要有一套标准。于是想到，如果有一个数据集收集了人类专家按照最佳实践分析数据得到的高质量结果，与之对比就能判断自动化方法的优劣。我们首先找到CMU的一位计算生物学教授，邀请她加入这个课题，希望能组织她教授的一门研究生课上的学生来共同打造这个基准数据集。在一次视频会议里，我向她展示了这个任务需要的分析流程后，她摇了摇头，说这个太专业化了（specialized），作为课程项目来说对学生的挑战太大。我们只好在校内组织一个团队来建造这个数据集。我们很快就发现那个教授说得没错，我们发现甚至网上的样例代码都不容易搜到，找到的寥寥几个有用的例子都是基于R的。于是我们这些CS背景的人通过看书、看网课，请教生物专家，恶补这方面的知识，一边召集团队，等了很久才在本校凑齐一个有足够计算生物背景的队伍，来建这个数据集。

基准数据集建好之后，就是智能体的研发。我们先试了当时最强的大模型和最先进的智能体。那会还是GPT-4时代，不出所料，所有方法和模型都无法生成代码来跑通从原始数据到分析结果的全流程，更不用说准确率了。现在的情况好了很多，随着大模型能力的飙升，我们发现从Claude 4开始，代码跑通已经不成问题了。但我们测试现有的先进智能体时，发现他们仍然经常犯隐蔽而严重的错误，而使得整个分析得到的结果科学上无效 (在论文附录A有详细的分析和例子)。

自动化方法在这里遇到的困难，植根于大模型自身的特点。首先，作为统计模型，它完成任务的表现高度依赖于任务相关互联网数据的高频性。它在众多学科展现出的强大能力，也源于互联网上大量的教材、习题等语料。除了解数学题、算法题等相对容易靠强化学习后训练提升的领域以外，真实世界大部分专业性和开放性强的任务中，大模型都会面临分布外泛化的严峻挑战。如果任务足够专业化，就像一个new grad到了公司里需要从头接收培训才能上手的任务，大模型往往也难以正确完成。其次，大模型是把所有信息以字符串的形式一股脑输入attention处理，这难以应对需要从若干个几十至几百兆的文件中整合数据、分析判断的基因表达分析任务。当上下文窗口过长，往往会淹没重要信息，严重降低大模型的任务表现。人的“工作记忆”远不如大模型的上下文窗口，但人会主动探索，会使用工具，也会选择对哪些输入信息进行思维深加工。一个熟练的生信专家只需要在流程的几个节点从大型文件里阅读关键的几十行，就能高质量完成整个分析任务。

[设计理念：可信的科学自动化，需要结合智能体和工作流的优势]

有了上面的思考，我们的目标就很明晰了：在智能体层面弥补大模型的不足，一是给它足够的领域知识帮它完成“入行培训”，规范它行为的同时，让它可以在此基础上，在任务中自己积累经验；二是在规划模块的层面上让它自主探索环境，选择性地处理和整合数据。探索就不可避免会犯错，所以我们要让它能即时从错误中恢复，不随着上下文积累变长后加剧的”自我条件”效应走入死胡同。我们在设计智能体的过程中，阶段性地拿我们的基准数据集评估性能，不断迭代，力求研发一个可信的全自动化基因表达分析任务的智能体。

这就引发一个问题，什么是可信的科学自动化？也许很多人认为，现在的大模型已经足够强大和智慧了，潜力无限。我们只要进一步提升它的agentic能力，再让它学会使用外部工具，那么不久后将来的某一天，它就能可信地自动进行科研工作。虽然这些是非常重要且激动人心的技术路线，可我们认为，即使大模型可以变得非常强大智慧，也不一定“可信”，这是因为，对于科学或者说科研，大家并没有一致认同的标准答案。一个学科里会有观点相左的不同学派，不同实验室对于科研的理解和规则也会有差异。在执行重要任务时，这些差异会体现在操作细则的各个方面，而且越是重要任务，越缺少妥协的余地。倘若向全世界最强的智能体丢一句简单的命令，让它根据自己的想法全自动做科研，再把结果端给用户，恐怕绝大部分用户是不会乐意的--如果ta是个严肃的科研工作者的话。

所以我们认为，可信的科学自动化，首先要让用户选择自己信任的guidelines。这个guidelines也许只是一个具体任务的简单指示，也许是复杂得多的一组文件。对于要进行的任务来说，这个guidelines要包含足够的文本信息让用户确信自己认同它将在这个任务中体现的科研风格、学派立场等等，于是作者选择相信，并对产生的结果愿赌服输。而agent这一侧，则需要确保按照用户信任的guidelines工作，同时也要对实际执行中的各种问题自主地探索解决方案，灵活处理，避免像人为编排的工作流那样僵化。因此，一个可信的智能体是需要结合agent和workflow优势的。

当然，以上只是概念上的思考。由于大模型的概率本质，我们无法确保智能体100%的遵守guidelines，而是通过智能体的编排，在大模型指令遵守和减少幻觉方面进展的基础上再进一步。

[方法设计、实验结果等部分]

[最后一段进行收束和展望]