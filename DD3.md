# 当数据蒸馏遇上几何学与鲁棒性：让AI训练又快又稳的两个新思路

深度学习时代，数据规模与算力消耗成为一对始终纠缠的矛盾。数据集蒸馏（Dataset Distillation）提供了一个颇具吸引力的解决方案：能否将庞大的训练集压缩为极少量的"精华样本"，同时让模型依然保持高性能？这不仅关乎算力节约，更触及了机器学习的本质问题——什么样的数据才真正重要？本文将介绍两个最新进展：一个从最优传输理论出发，用几何的方式思考数据压缩；另一个从损失曲面的平滑度入手，在蒸馏中植入对抗鲁棒性。这两项工作分别发表于 ICCV 2025 与 AAAI 2025，为数据蒸馏的研究开辟了新的视角。

## 从最优传输到数据蒸馏：WMDD 的几何学视角

> 论文：Dataset Distillation via the Wasserstein Metric（ICCV 2025）  
> arXiv：https://arxiv.org/abs/2311.18531

传统的数据蒸馏方法大多采用梯度匹配或轨迹对齐等技巧，试图让合成数据在训练动态上"模仿"真实数据。但这类方法往往面临计算复杂度的困扰，特别是在 ImageNet 这样的大规模数据集上，二阶梯度的计算几乎难以承受。另一条路径是分布匹配，用最大均值差异（MMD）等度量直接拉近真实与合成数据的分布。然而 MMD 的表现常常不尽如人意——它严重依赖核函数的选择，而在高维特征空间中，什么样的核才是"对"的，本身就是一个难题。

WMDD 的创新在于引入了 Wasserstein 距离这一来自最优传输理论的工具。Wasserstein 距离衡量的是将一个概率分布"搬运"到另一个概率分布所需的最小代价，这种度量天然地保留了分布的几何结构。打个比方，如果把数据分布看作空间中堆积的沙土，那么 Wasserstein 距离关心的是如何用最小的功把一堆沙土重新堆成另一堆的形状。这种"搬运"的视角，恰好契合数据蒸馏的需求：我们希望少量合成样本不仅在统计量上接近真实数据，更要在空间结构上保持相似。

具体而言，WMDD 在预训练分类器的特征空间中操作。对于每个类别，真实样本被映射为特征向量的集合，算法计算这些特征的 Wasserstein 重心——一个由若干带权"锚点"组成的分布，它以最优传输的意义最接近所有真实特征。合成图像的优化目标很直接：让它们的特征尽可能贴近这些锚点。为了更好地利用预训练模型中的信息，WMDD 还引入了按类别分组的 BatchNorm 统计约束（PCBN）。与全局 BN 正则不同，PCBN 在每个类别内部分别对齐 BN 的均值与方差，避免了不同类别样本在优化时相互干扰，从而更好地保持类内的多样性和类间的区分度。

![WMDD：不同分布度量下的"重心"对比（示意）](DD_figs/WMDD_different_metrics.jpg)

_图 1  在二维玩具示例中，Wasserstein 重心（右）相比 KL 散度和 MMD，更自然地插值了原始分布的形状，保留了圆环与交叉的几何特征。_

为什么 Wasserstein 在实践中表现更好？一方面，它提供了理论保障：在适当的 Lipschitz 连续性假设下，真实分布与合成分布在期望风险上的差距可以被 Wasserstein-1 距离所界定。另一方面，计算 Wasserstein 重心的复杂度关于真实数据量是线性的，远低于完整核化 MMD 的平方复杂度。许多 MMD 方法为了规模化不得不退而求其次，仅对齐特征均值（相当于使用线性核），这在统计上无法唯一确定分布，丢失了高阶信息。而 Wasserstein 则在保持可计算性的同时，捕捉到了分布的全局几何特征。

实验结果令人振奋。在 ImageNette、Tiny-ImageNet 和 ImageNet-1K 三个数据集上，WMDD 在 1、10、50、100 IPC（每类图像数）的各种设置下均达到或超越现有方法。特别值得注意的是跨架构泛化能力：用 ResNet-18 蒸馏的数据，在训练 ResNet-50/101 甚至 Vision Transformer 时依然有效，这表明 WMDD 学到的不是某个特定网络的"捷径"，而是真正抓住了数据的本质结构。效率方面，WMDD 与当前最快的分布匹配方法处于同一量级，但准确率往往显著领先。

![WMDD：方法示意](DD_figs/WMDD_method_diagram.jpg)

_图 2  WMDD 流程：在特征空间计算 Wasserstein 重心，用特征匹配损失和按类 BN 正则共同优化合成图像。_

消融实验进一步验证了设计的合理性。将特征匹配目标从交叉熵换成 Wasserstein 重心回归，在所有数据集上都带来增益；将全局 BN 改为 PCBN，类内特征不再"塌缩"，多样性得以保持。有趣的是，使用 Sliced Wasserstein 作为更快速的近似，精度几乎不损，这证明了核心收益确实来自 Wasserstein 几何本身，而非某个精细调校的超参数。

WMDD 的意义不仅在于刷新了基准数据集的指标，更在于为数据蒸馏提供了一个几何学的视角。未来可以探索的方向包括：在联合空间 P(X, Y) 中同时蒸馏输入与标签，以捕捉标签之间的语义关系；将 Wasserstein 与生成模型结合，进一步提升类内多样性；甚至考虑在决策边界附近采样"支撑点"，让合成数据不仅贴近分布重心，也突出关键的判别区域。

![WMDD：蒸馏样本示例](DD_figs/WMDD_distilled_images.jpg)

_图 3  WMDD 在 ImageNet-1K 10 IPC 设置下合成的图像，每个类别随机采样 1 张。可以看到合成图像保留了类别的关键视觉特征。_

## 从曲率正则到对抗鲁棒：GUARD 的损失景观视角

> 论文：Towards Adversarially Robust Dataset Distillation by Curvature Regularization（AAAI 2025）  
> arXiv：https://arxiv.org/abs/2403.10045

数据蒸馏的研究大多聚焦于提升干净数据上的准确率，但一个同样重要的问题往往被忽视：在蒸馏数据上训练的模型，面对对抗攻击时还能稳定吗？对抗鲁棒性是可信机器学习的基石，然而在数据蒸馏场景下，如何获得鲁棒模型并非显而易见。

最直接的想法是将对抗训练嵌入蒸馏流程：用对抗样本训练教师模型，再蒸馏数据。但实验表明，这种做法往往事与愿违——干净精度大幅下降，鲁棒性提升也不稳定。原因在于对抗训练会显著改变样本的语义，在本就极度压缩的蒸馏数据上，这种语义扰动被放大，导致分布偏移严重。那么，有没有一种更温和、更本质的方式来植入鲁棒性？

GUARD（Geometric Regularization for Adversarially Robust Dataset）从损失曲面的几何性质入手。理论分析表明，对抗损失的上界由三部分组成：标准损失、梯度幅度和 Hessian 矩阵的最大特征值（即最大曲率）。直觉上，如果损失曲面在输入邻域内陡峭、弯曲，那么微小的对抗扰动就能让损失急剧上升；反之，如果曲面平滑，模型对扰动的敏感度就会降低。更重要的是，当蒸馏数据与真实数据在特征空间充分接近时，二者的对抗损失上界仅相差一个与分布偏差成正比的常数项。这意味着，只要蒸馏质量足够好，在蒸馏数据上优化鲁棒性就能可靠地迁移到真实数据上。

基于这一洞察，GUARD 的做法非常简洁：在蒸馏过程中对损失曲率施加正则化。具体而言，对于输入 x，我们在其梯度方向上做一个小步长扰动 hz（其中 z 是归一化梯度），然后最小化扰动前后梯度的差异。这一正则项实际上是在近似降低 Hessian 的最大特征值，使损失在梯度方向（往往也是最大曲率方向）上更接近线性。在 SRe2L 等蒸馏框架中，GUARD 只需在 squeeze 阶段将标准损失替换为"原损失 + 曲率正则"即可，每次迭代仅多一次前向传播和梯度计算，没有内层对抗优化的循环，代价极低。

![GUARD：蒸馏样本示例](DD_figs/GUARD_distilled_images.jpg)

_图 4  GUARD 在 ImageNet-1K 1 IPC 设置下的蒸馏图像。这些合成样本融合了类内多个对象的特征，体现了对核心语义的捕捉。_

效果如何？在 ImageNette、Tiny-ImageNet 和 ImageNet-1K 上，GUARD 在多种白盒（PGD、AutoAttack）和黑盒（Square、CW）攻击下都显著提升了鲁棒性，而且常常"附带"提升干净精度。例如在 ImageNette 10 IPC 上，干净准确率从 42.42% 跃升至 57.93%，AutoAttack 的鲁棒准确率从 4.99% 提升到 19.69%；在 Tiny-ImageNet 50 IPC 上，PGD100 的鲁棒率从 0.27% 提升到 15.63%，AutoAttack 从 0.16% 提升到 13.84%。更令人惊喜的是，GUARD 作为一种通用正则化手段，可以无缝嫁接到 DC、CDA 等不同蒸馏范式上，均能同时改善精度和鲁棒性。

为什么曲率正则能同时提升精度和鲁棒性？背后的直觉是：崎岖的损失曲面会让蒸馏过程"看到"不稳定的梯度信号，合成样本容易捕捉高频噪声而非稳定的类别特征；平滑曲面则引导蒸馏朝向更泛化的方向。实验中还发现，在 IPC 较小时（如 1 或 10），曲率正则带来的双重提升尤为显著；IPC 较大时，过强的正则可能抑制细粒度判别，此时适当调低正则强度即可平衡。这暗示了一个有趣的现象：合成数据量越少，模型复杂度（体现为曲率）应越低，但如何从理论上严格阐释这一点，仍是开放问题。

需要强调的是，GUARD 并非提供形式化的全局鲁棒保证，一些假设（如局部凸性、Lipschitz 连续性）在实际中是近似成立的。但它以极小的计算代价，在大规模数据和强攻击场景下给出了精度与鲁棒兼顾的实用方案，并且具有良好的可迁移性。

## 几何与鲁棒性：数据蒸馏的两个维度

WMDD 与 GUARD 分别从"空间结构"与"对抗稳定性"两个维度推进了数据蒸馏的研究。前者借助最优传输理论，用几何的眼光重新审视分布匹配，让合成数据不仅在统计量上接近真实数据，更在形状上保形；后者从损失曲面的平滑度出发，用简洁的曲率正则在蒸馏中植入鲁棒性，避免了对抗训练的高昂代价。这两项工作共同揭示了一个更深层的图景：数据蒸馏不是简单的"样本压缩"，而是对数据本质特征的提炼与重构——既要在几何上保真，也要在扰动下稳健。

未来的研究可以沿着多个方向延伸。在 WMDD 方向，值得探索如何在联合空间中同时蒸馏输入与标签，捕捉标签间的语义关系；如何将 Wasserstein 与生成模型（如扩散模型）结合，进一步提升合成数据的多样性和真实感。在 GUARD 方向，可以探索自适应调节曲率正则强度以适配不同 IPC，或将其推广到分布外泛化、多模态学习等更复杂场景。更宏观地看，数据蒸馏有望成为走向高效、可信 AI 的一条重要路径，而几何学与鲁棒性的深度融合，将为这一领域注入更多理论洞察与实用价值。

两篇论文的代码均已开源：  
WMDD: https://github.com/Liu-Hy/WMDD  
GUARD: https://github.com/yumozi/GUARD

期待这些工作能为数据蒸馏的研究提供新的灵感，也欢迎更多研究者加入这一充满挑战与机遇的领域。