在数据越来越多、模型越练越大的今天，“数据蒸馏”让我们看到一条优雅但务实的路：把海量训练集压缩成每类仅几张的“精华集”，再用它们训练出精度不输全量数据的模型。然而，效率之外，可信赖性同样关键——当模型面对对抗攻击时，训练在蒸馏数据上的模型还能扛得住吗？我们在 AAAI 2025 的新作提出：不仅要“压缩”数据，更要把“鲁棒性”一起压进去。为此，我们给出一种简单高效、带有理论支撑的做法：通过“曲率正则”在蒸馏过程中直接塑造可迁移的对抗鲁棒性。我们将其命名为 GUARD（Geometric Regularization for Adversarially Robust Dataset）。

直觉上，最自然的办法是把对抗训练嵌入蒸馏流程：用鲁棒模型作“教师”，以此提炼更抗攻击的合成样本。但实践却给出了相反信号。我们在 SRe2L 框架上做了一个小心翼翼的例子：只用非常弱的 PGD（ϵ=1/255）参与蒸馏阶段的对抗训练，得到的蒸馏数据在干净测试上的性能却显著崩塌，例如 ImageNette 10 IPC 的干净准确率从 42.42% 跌到 12.81%，鲁棒性也并不稳定。这并不意外——已有研究指出对抗训练会改变图像的语义表征，蒸馏又极度压缩了分布，两者叠加反而容易把“精华”扭曲掉。这促使我们转向更本质的提法：不是在外层堆叠一个对抗环路，而是从损失几何出发，直接改造模型面对真实数据时的“局部曲率”。

核心理论结论非常朴素却有力：把样本在小球扰动内的对抗损失近似展开，可以得到一个上界，其中最重要的项来自损失对输入的“最大曲率”（Hessian 最大特征值）。在假设蒸馏数据已能较好贴近真实分布（特征距离 σ 足够小）、且最优点附近梯度项影响相对次要的情况下，这个上界的主导因子就是曲率本身。换句话说，只要我们让模型在真实数据邻域的损失面更“平”，训练在蒸馏数据上的模型就能继承到更强的对抗鲁棒性。这一视角的一个重要副产物是可迁移性保证：真实数据与蒸馏数据上的对抗损失上界只相差一个与 σ 成正比的常数项，因此“在蒸馏数据上优化鲁棒性”能够可靠地转移到“在真实数据上评测鲁棒性”。

基于此，我们提出 GUARD 曲率正则。直接算 Hessian 最大特征值代价太高，我们借助经验事实：梯度方向与最大曲率方向在输入空间往往高度相似。于是我们用单位梯度方向做一个小步长扰动，最小化两处梯度的差异范数，让损失面在这一方向上尽量线性化。这等价于最小化曲率的高效近似。落地实现上，我们把这项正则嵌入 SRe2L 的 squeeze 阶段，将其标准训练损失替换为“原损失 + 曲率正则”，每个迭代仅多一次前向与梯度计算即可，无需引入内层对抗环路。测得的代价非常低：在 A100 80GB 上每步仅约 0.007s，峰值显存约 3.8GB，对比把对抗训练硬塞进蒸馏要 2.198s/步与更高的显存占用。工程上，GUARD 更像一个“即插即用”的小组件，而不是一个重工程的三层优化器。

实验方面，我们在 ImageNette、Tiny ImageNet 和 ImageNet-1K 上系统评估，统一用 ResNet-18 进行蒸馏与验证，并覆盖 PGD100、Square、AutoAttack、CW、MIM 等多种白盒/黑盒攻击。在多个蒸馏规模（10/50/100 images per class）下，GUARD 在鲁棒指标上广泛优于强基线，并常常“顺带”提升干净精度。例如，在 ImageNette 10 IPC 上，GUARD 的干净准确率为 57.93%，对比原 SRe2L 的 42.42%；在同设下，PGD100 从 4.76% 提升到 23.87%，AutoAttack 从 4.99% 提升到 19.69%。Tiny ImageNet 的 50 IPC 场景中，PGD100 从 0.27% 显著提升到 15.63%，AutoAttack 从 0.16% 提升到 13.84%。在更具挑战的 ImageNet-1K 10 IPC 上，干净准确率由 21.30% 提升到 27.25%，PGD100 从 0.55% 提升到 5.25%。当压缩比例放宽到 ImageNette 100 IPC，PGD100 由 31.65% 提升到 57.50%，AutoAttack 由 17.93% 提升到 64.84%。这些结果揭示了一个耐人寻味的规律：通过让损失几何更“平”，我们不仅把鲁棒性“蒸”了进去，还让模型对干净样本的泛化更稳了。这与正则化帮助泛化的一般认识相呼应。

我们也认真做了对比与消融。首先，把“梯度范数”本身当作正则（而不是曲率差分）并不能带来同等的鲁棒提升，说明“针对曲率”的几何塑形是关键。其次，直接在蒸馏里做对抗训练的替代方案不仅算力高、内环复杂，还容易出现我们前面提到的语义扭曲与分布错配，实验数据清楚地显示它对干净精度造成了灾难性打击且鲁棒收益并不稳定。最后，我们验证了 GUARD 的可迁移性与普适性：把它加到 DC 与 CDA 这样的不同蒸馏范式上，在 CIFAR-10 的 1 / 10 / 50 IPC 各种设定下，GUARD 也能同时提升干净与鲁棒性能。这支持了我们的设计初衷——GUARD 是一个可以贴合多种蒸馏框架的轻量几何正则，而不是某一特定算法的“私有优化”。

从机制直觉看，为什么“把曲率压平”能帮助蒸馏鲁棒性？对抗攻击本质上在输入的局部邻域里寻找能迅速增大损失的方向；当损失面在真实数据邻域足够平滑，梯度变化不剧烈，攻击步伐便难以“踩住”快速上升的坡。这种几何上的稳定性天然会被蒸馏出的样本继承，从而在小数据训练下也能保住决策边界的稳健。此外，我们的理论还指出真实数据与蒸馏数据在对抗损失上界的差距只体现在一个与特征分布偏差 σ 成正比的项上；当蒸馏已较好对齐真实分布时，这项偏差可控，这也解释了为何“在蒸馏数据上学到的鲁棒性”能迁移到真实验证集上。

当然，我们并不宣称这是一种形式化可证的全局鲁棒保证。理论推导采用了合理但理想化的假设（如局部凸性、特征映射的 Lipschitz 等），因此它提供的是“方向性的指导与上界控制”，而非逐点的最优证书。但正如大量实证所示，这一几何约束在实际深度网络的局部区域是行之有效的：它既避免了对抗训练那样的三层优化开销，也在大规模数据与强攻击设置下给出了兼顾精度与鲁棒的均衡解。

我们相信，GUARD 提供的是一种“把鲁棒性也蒸馏出来”的新范式。面向算力受限的边端与隐私敏感的场景，它可以显著降低训练成本；面向大模型与大数据，它以极低的增量开销，把“强健的损失几何”注入训练流程；面向方法学发展，它以清晰的几何解释连接了蒸馏、鲁棒与泛化这三者。论文与代码已开源，欢迎检阅与复现：https://github.com/yumozi/GUARD。我们也期待社区把这类曲率正则进一步推广到更多蒸馏家族甚至分布外鲁棒场景，让“小而强”的数据集成为通往可信 AI 的一条可持续道路。