# 当数据蒸馏遇上几何与鲁棒性：两条平行路径的交汇

大模型训练的成本正在变得令人望而却步。当一个 ImageNet 规模的数据集需要数天甚至数周才能完成一轮训练时，我们不得不思考：是否存在一种更经济的方式，让模型在保持性能的前提下大幅降低训练开销？数据集蒸馏（Dataset Distillation）给出了一个颇具前景的答案——将海量训练数据压缩成极少量的合成样本，却依然能训练出性能相当的模型。然而，这个看似简洁的想法背后，隐藏着两个本质性的难题：蒸馏后的数据能否保持原始分布的本质特征？训练出的模型能否经受住对抗攻击的考验？我们最近的两项研究，分别从分布几何与对抗鲁棒性两个角度，对这些问题给出了新的答案。

## 从最优传输理论出发：让蒸馏数据"记住"分布的形状

> WMDD：Dataset Distillation via the Wasserstein Metric (ICCV 2025)  
> 论文链接：https://arxiv.org/abs/2311.18531

数据蒸馏的本质，是一个信息压缩问题。当我们试图用每类仅仅 10 张甚至 1 张合成图像去概括成千上万真实样本时，什么信息最值得保留？许多现有方法选择匹配梯度、轨迹或者简单的统计量，但我们认为，真正应该被保留的是数据分布的几何结构——那些决定类别间界限、类内变化模式的空间关系。

Wasserstein 距离为我们提供了一个理想的工具。这个源自最优传输理论的度量，最早可以追溯到 18 世纪法国工程师 Monge 关于土方搬运的思考：如何以最小的代价将一堆土重新分布成另一种形状？这个朴素的问题，经过 Kantorovich 等数学家的发展，最终演化成现代优化理论中优雅而深刻的分支。Wasserstein 距离的特别之处在于，它不是简单地比较两个分布的某些统计量，而是计算将一个分布"搬运"成另一个分布所需的最小总功。这意味着，它天然地编码了数据点之间的几何关系——哪些样本彼此接近，哪些相距较远，整个分布呈现出什么样的形态。

当我们用 Wasserstein 距离来定义分布的"重心"时，得到的不是若干分布的简单平均或僵硬混合，而是在几何意义上同时接近所有输入分布的一个中心形态。想象在二维平面上有两类数据，一类分布在圆环上，另一类呈十字形。如果用传统的 KL 散度或 MMD 来计算重心，你会得到一团模糊的混合；而 Wasserstein 重心则能保留圆环和十字的结构特征，以最自然的方式实现几何插值。这种"保形"能力，恰恰是数据蒸馏最需要的。

WMDD 方法的核心思路，是将蒸馏过程搬到预训练模型的特征空间进行。我们先用一个在原始数据上训练好的分类器，将每个样本映射到倒数第二层的特征表示；然后针对每个类别，在特征空间中计算其 Wasserstein 重心。这个重心不是单个点，而是由若干带权重的"锚点"组成，每个锚点代表类内分布的一个重要模式。接下来，我们通过梯度优化合成图像，使其特征表示尽可能贴近对应类别的重心锚点。

然而，仅仅匹配特征还不够。我们注意到，预训练模型的 BatchNorm 层隐含地编码了数据分布的重要先验——每一层特征的均值和方差，反映了该层对不同类别数据的响应模式。传统方法倾向于在全局层面对齐 BN 统计量，但这会导致一个问题：来自不同类别的样本在同一个 batch 中混合，优化某个样本时产生的梯度，会受到其他类别样本的干扰。我们提出的按类 BatchNorm（Per-Class BN, PCBN）正则化方法，选择在类内单独对齐 BN 统计量，让每个类别的合成样本独立地学习其特征分布的均值和方差，避免了类间梯度的相互牵扯。

为什么 Wasserstein 方法能在大规模数据集上既高效又有效？这里有两个关键点。从理论层面，Wasserstein 距离提供了更紧的泛化界：在合理的 Lipschitz 连续性假设下，真实分布和合成分布之间的期望风险差异，可以直接被 Wasserstein-1 距离所上界。从实践角度，许多基于 MMD 的方法为了可扩展性，不得不退化为仅匹配特征均值——这等价于使用线性核的 MMD。问题在于，线性核并非特征核（characteristic kernel），仅匹配均值无法唯一确定分布，会丢失高阶矩和几何结构信息。完整的核化 MMD 虽然理论上更优，但计算复杂度随数据量平方增长，在 ImageNet 级别根本无法实施。而 Wasserstein 重心计算虽然看似复杂，实际上可以通过交替优化位置和权重来高效求解：固定位置时优化权重是一个线性规划问题，可以用对偶形式快速求解；固定权重时优化位置，每个锚点的更新可以通过一步 Newton 法完成。整个过程的复杂度仅随数据量线性增长，配合特征空间的降维，在 ImageNet-1K 上依然可以高效运行。

实验结果充分验证了这一设计的有效性。在 ImageNette、Tiny-ImageNet 和 ImageNet-1K 三个数据集上，WMDD 在 1 到 100 IPC 的各种压缩比下都达到了当前最优或极具竞争力的性能。特别值得关注的是跨架构泛化能力：用 ResNet-18 蒸馏得到的数据，在 ResNet-50 和 ResNet-101 上训练时性能持续提升，甚至在 Vision Transformer 这种结构差异巨大的模型上也能取得不错的表现。这说明 WMDD 捕捉到的不是某个特定网络架构的"捷径"，而是真正贴近原始数据分布的几何本质。

消融实验进一步揭示了设计选择的合理性。将特征匹配损失从交叉熵替换为 Wasserstein 重心距离，在所有数据集上都带来一致的提升；引入 PCBN 后，合成图像的类内多样性得到更好保留，不同样本的特征不再坍缩到同一点。我们还测试了 Sliced Wasserstein 距离作为更快的近似，结果显示精度几乎没有损失，证实了核心收益确实来自 Wasserstein 几何本身，而非某些精心调校的超参数。

从方法论的视角看，WMDD 将数据蒸馏问题重新锚定在分布几何的基本框架中：通过最优传输的视角，我们不再满足于匹配某些离散的统计量或梯度信号，而是直接追问如何以最小代价将合成分布塑造成真实分布的形状。这种思路为后续研究打开了新的可能性：例如在联合空间 P(X,Y) 上直接蒸馏，同时保留输入特征和标签之间的关系结构；或者将全局的重心对齐与局部的判别边界结合，让部分合成样本更靠近决策边界，以提升模型的判别能力。

## 从损失曲率入手：让蒸馏模型经受住对抗攻击

> GUARD：Towards Adversarially Robust Dataset Distillation by Curvature Regularization (AAAI 2025)  
> 论文链接：https://arxiv.org/abs/2403.10045

如果说 WMDD 关注的是"保真度"，那么另一个同样重要的问题则是"稳健性"。在蒸馏数据上训练的模型，面对精心设计的对抗扰动时能否保持可靠？这不仅关乎模型的安全性，也反映了蒸馏数据是否真正捕捉到了类别的本质特征——那些不会被细微扰动所动摇的稳定模式。

对抗鲁棒性的传统做法是对抗训练：在训练过程中不断生成对抗样本，强迫模型学会抵御这些扰动。将这个思路直接搬到数据蒸馏中似乎很自然——用对抗训练得到的鲁棒模型来指导蒸馏过程。但实验很快给出了否定的答案：嵌入对抗训练后，蒸馏数据的干净准确率急剧下降，鲁棒性的提升也极不稳定。问题出在哪里？对抗训练通过添加扰动来改变样本的语义，而在数据本就极度稀缺的蒸馏场景下，这种语义漂移会被放大，导致合成样本的分布与真实数据严重偏离。

我们需要一条不同的路径。GUARD 的核心洞察是：对抗鲁棒性从根本上取决于损失函数的局部几何。如果将损失函数在某个样本附近做二阶泰勒展开，对抗损失的上界由三项决定——原始损失、梯度幅度和 Hessian 矩阵的最大特征值（即最大曲率）。其中，最大曲率主导了对抗损失的增长：曲率越大，损失函数在某些方向上越"陡峭"，对抗扰动就越容易让损失快速上升。

更进一步，我们的理论分析表明，只要蒸馏数据与真实数据在特征空间足够接近（这正是数据蒸馏的基本假设），那么在真实数据上降低损失曲率，就能够有效控制在蒸馏数据上训练的模型的对抗损失上界。两者之间的差异仅仅是一个与分布偏差成正比的常数项。这个发现打消了一个关键的疑虑：我们不必担心在蒸馏数据上优化鲁棒性是否能够迁移到真实数据的评测中——理论保证了两者是紧密耦合的。

但是，如何高效地降低最大曲率？直接计算 Hessian 矩阵的最大特征值在计算上是禁止性的。我们借助一个经验观察：神经网络的梯度方向往往与最大曲率对应的特征向量高度对齐。因此，我们可以用梯度方向作为曲率方向的近似。具体做法是，沿着归一化的梯度方向做一个小步长的扰动，然后最小化扰动前后梯度的差异。这个正则项鼓励损失函数在梯度方向上更加平滑，从而间接降低最大曲率。

实现上，我们将这个曲率正则项嵌入到 SRe2L 蒸馏方法的"squeeze"阶段。每次迭代只需要额外计算一次前向传播和梯度，相比于对抗训练的内层优化循环，计算开销微乎其微——实测显示每次迭代的时间仅增加约 0.3%，而对抗训练则会增加超过 300 倍。

实验结果令人振奋。在 ImageNette、Tiny-ImageNet 和 ImageNet-1K 上，GUARD 在各种 IPC 设置下都显著提升了对抗鲁棒性，同时——出乎意料地——还提升了干净准确率。以 ImageNette 10 IPC 为例，干净准确率从 42.42% 跃升至 57.93%，面对 AutoAttack（一种集成了多种强攻击的评测套件）的鲁棒率从 4.99% 提升到 19.69%。在更具挑战的 ImageNet-1K 50 IPC 设置下，面对 PGD100 攻击的鲁棒率从几乎为零提升到接近 10%。这种干净准确率和鲁棒性的双重提升，表明 GUARD 不仅仅是在"加固"模型，而是真正改善了蒸馏数据的质量。

为什么平滑损失曲率能带来这样的效果？直觉上，高曲率的损失景观意味着梯度方向在空间中快速变化，蒸馏过程从这样的教师模型中提取信息时，会受到高频噪声的干扰，难以稳定地恢复出类别的核心特征。降低曲率后，损失景观变得更加平缓，梯度信号更加一致，蒸馏得到的合成样本更倾向于捕捉那些稳定的、不受局部扰动影响的特征模式——而这些特征恰恰是同时有利于干净泛化和对抗鲁棒的。

实验还揭示了一个有趣的现象：在较小的 IPC（如 1 或 10）下，曲率正则对干净准确率和鲁棒性都有显著提升；但当 IPC 较大时，过强的正则可能抑制细粒度的判别信息，需要适当减弱正则强度。这暗示了一个值得深入探索的方向：合成数据量越少，能够承载的模型复杂度应该越低。如何理论化地刻画这个权衡，以及如何自适应地调整曲率约束以适配不同的 IPC，是未来工作的重要课题。

GUARD 的另一个优势是其通用性。作为一个损失景观层面的正则化方法，它可以无缝集成到几乎所有使用教师模型指导蒸馏的方法中。我们在 DC、CDA 等不同范式的蒸馏方法上进行了测试，都观察到了一致的改进。这表明，曲率正则揭示的是数据蒸馏中一个更深层的原理，而非针对某个特定方法的技巧。

需要澄清的是，GUARD 并不宣称提供形式化的全局鲁棒性保证。我们的理论基于一些理想化的假设，如局部凸性和 Lipschitz 连续性。但在实践中，它以极低的计算代价，在大规模数据集和强攻击设置下实现了精度与鲁棒性的良好平衡。

## 两条路径的交汇：精准与稳健的协同

这两项工作从不同的角度切入数据蒸馏的核心挑战，却指向了一个共同的方向：让神经网络从蒸馏数据中学到更本质、更稳定的表示。WMDD 通过最优传输理论的几何视角，确保蒸馏数据在表示空间中准确地反映真实分布的形状；GUARD 通过平滑损失景观，确保模型学到的特征不会被局部扰动所动摇。两者的结合，有望让数据蒸馏在"少而精"的基础上，进一步实现"少而稳"。

展望未来，这两个方向都有丰富的延伸空间。对于 WMDD，一个自然的问题是如何在联合空间 P(X,Y) 上直接蒸馏，同时保留输入和标签之间的关系结构；另一个有趣的方向是将全局的重心表示与局部的判别边界结合，让合成样本既能概括分布的主体，又能捕捉决策边界附近的关键信息。对于 GUARD，未来的工作可以探索更精细的自适应机制，根据不同的 IPC 和数据特性动态调整曲率约束；也可以将这个思路推广到分布外泛化和多模态学习等更广泛的场景。

更宏观地看，数据蒸馏作为一种数据压缩技术，其价值不仅在于降低计算成本，更在于迫使我们重新审视：什么样的信息对于学习真正重要？在追求极致压缩的过程中，我们逐渐剥离掉数据中的冗余和噪声，留下的是那些对泛化和鲁棒性都至关重要的核心模式。从这个意义上说，数据蒸馏不仅是一个工程问题，更是一个认识论问题——它帮助我们理解，机器学习究竟在学什么。

两篇论文的代码均已开源：  
WMDD: https://github.com/Liu-Hy/WMDD  
GUARD: https://github.com/yumozi/GUARD

我们期待这些工作能为构建更高效、更可信的 AI 系统贡献一份力量。在大模型时代，"小而强"的数据集或许能成为一条更加可持续的发展路径。
