把“数据的形”保留下来：WMDD 用 Wasserstein 度量重塑数据蒸馏

数据蒸馏正在成为大模型时代里一条越来越现实的“降本增效”路径。它希望用极少量的合成样本，训练出与完整大数据集相当的模型性能，从而让研究与部署在时间、成本、能耗上都更可承受。但如何在极其有限的样本中，保留住一个复杂数据分布的关键信息与内部结构，一直是这条路线的难点。我们近期在 ICCV 2025 接收的工作 WMDD（Wasserstein Metric-based Dataset Distillation）尝试从最本源的角度出发：直接用最能刻画“形状”的距离来做分布对齐——Wasserstein 距离，并以其“重心”（barycenter）作为蒸馏的目标。

直观地说，如果把数据分布想象成一团可以流动的“质量”，Wasserstein 距离衡量的是把一团搬运成另一团所需的最小“功”；它关心的不只是均值或协方差，而是带着几何结构地对齐每一份质量。与此对应的 Wasserstein 重心，不是简单把若干分布叠加混匀，而是在几何意义上找到能同时接近这些分布的“中心形态”。我们在论文中用一个二维玩具例子对比了 KL、MMD 与 Wasserstein 重心：前两者更像把圈和叉“糊”在一起，而后者则在形状上保留了各自的结构特征。这种“保形”的能力，正是我们认为数据蒸馏最需要的。

基于这一洞见，WMDD 将合成数据的学习搬到了特征空间里进行。做法很简洁：先用一个在原始数据上训练好的分类器，把每一类的样本映射到最后一层线性分类器之前的特征空间，再在这个空间中分别为每一类计算 Wasserstein 重心。注意这里的重心不是一个点，而是一组带权代表点；它们用尽量少的“锚”概括了类内分布的多样性与几何结构。随后，我们通过梯度下降优化每一类的少量合成图像，使其特征尽可能贴近对应的重心代表点。同时，我们引入了一个简单但关键的正则：按类统计的 BatchNorm 约束（Per-Class BN，PCBN）。以往做 BN 对齐时往往在全局上对齐，这会让不同类的梯度信号彼此牵扯，而我们改为在每一类内部，对齐其在各层 BN 的均值与方差，这样既利用了预训练网络携带的先验，又不打乱类内结构的学习过程。为保证可扩展性，我们采用了高效的最优传输解法，对重心的“位置—权重”交替优化，计算与存储开销与主流的分布匹配式蒸馏方法处于同一量级。

为什么这套方案能在高分辨率数据集上跑得动、还跑得好？除了几何直觉，我们也在附录中给出了一种可能的理论解释：在合理的 Lipschitz 假设下，真实分布与合成分布在期望风险上的差异，可以被 W1（Wasserstein-1）距离成比例地上界；而许多基于 MMD 的蒸馏实践为了规模化，退而只对齐特征均值（相当于线性核的 MMD），这在统计上并不能唯一标定分布，往往会忽略高阶差异与几何结构，从而在有限样本的 regime 下给优化提供不了足够“有形”的信号。当然，完整的 MMD 配上特征核也能做得更精确，但计算开销随样本成平方增长，很难在 ImageNet-1K 量级上落地。Wasserstein 重心在我们的方法里既保留了对结构的敏感性，又借助特征空间与高效 OT 求解保持了可计算性。

实验上，WMDD 在 ImageNette、Tiny-ImageNet 与 ImageNet-1K 三个高分辨率数据集上，1/10/50/100 IPC 的多种预算下都给出了强竞争甚至领先的结果。以 100 IPC 为例，我们在三套数据上的 top-1 分别达到约 87.1%、61.0% 和 60.7%，逼近用全量数据训练的同架构模型（约 89.9%、63.5%、63.1%）。更值得一提的是跨架构泛化：我们用 ResNet-18 蒸馏得到的合成数据，在 50 IPC 下训练 ResNet-50/101 还能持续增益，迁移到 ViT-Tiny/Small 也有可观表现，这说明 WMDD 学到的不是“过拟合某个骨干”的捷径，而是真正在特征几何上贴近了真实分布。效率方面，得益于重心计算的可解性，我们的时间与显存开销与当前最为高效的分布匹配方法处于同一数量级，在 RTX-3090 上 1 IPC 的端到端时间与 SRe2L 相仿，却在精度上实现了全面超越。

我们还做了一系列消融来拆解设计取舍。首先，把特征匹配从交叉熵替换为 Wasserstein 重心回归，在三个数据集上都带来稳定增益；其次，把全局 BN 正则改为 PCBN 后，类内多样性保持得更好，特征分布不再“塌缩”，与重心匹配目标形成共振。我们也尝试了 Sliced Wasserstein 作为更快的替代，得到的精度几乎不损，速度略有提升，说明 WMDD 的核心收益确实来自 Wasserstein 几何，而非某个具体求解器的工程技巧。超参方面，正则强度 λ 在一个宽区间内都很鲁棒；当 λ 过小，合成图像会出现高频伪影，像是在“迎合”某个模型权重而非分布本身，这与直觉一致。

从更宏观的视角看，WMDD 不只是一个“更快更准”的蒸馏器。它把数据蒸馏重新拉回到“分布几何”这一基本物理图景：用最小搬运代价去贴近真实数据的形，既不丢掉类内的多样性，也不破坏类间的相对关系。这种方式天然适合与预训练表征协同，也为下一步与生成式模型的结合打下了基础。我们相信，在大模型与大数据的夹缝中，如何“带着形状”做数据压缩，会是推动训练与部署绿色化、普惠化的一条重要路径。当然，任何对真实分布的忠实拟合都可能放大数据里已有的偏置，这需要在蒸馏过程中引入更细粒度的公平性约束与审计机制；但从能耗节约到算力门槛的降低，其综合社会效益同样不容忽视。

总之，WMDD 展示了一个简单而有效的方向：让度量回归几何，让蒸馏回归分布。在可计算的框架里保住“数据的形”，是我们这项工作的核心价值。欢迎关注我们在 ICCV 2025 的论文，代码与页面已开源，期待与学术与产业伙伴一起把这条路走得更远。