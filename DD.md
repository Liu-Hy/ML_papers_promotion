把“形”蒸出来，把“弯”抹平：WMDD 与 GUARD 的双线突破

引子
在大模型与大数据的博弈中，数据蒸馏提供了一条兼顾效率与效果的现实路径：用极少量、经过精心“压缩”的合成样本，训练出性能逼近甚至不输全量数据的模型。但仅有“省时省算力”还不够——真正可落地的系统，还需要在分布保真与安全鲁棒上同时过关。我们团队近期的两项工作分别从这两端发力：WMDD 让合成数据更“像”真实数据，尽可能保住“数据的形”；GUARD 则在蒸馏过程中把“抗攻击”的能力一并注入模型，把损失几何中恼人的“弯曲”抹平。本文在保留两篇原稿学术严谨与可读性的基础上，将它们整合为一篇更紧凑、更连贯的报道。

WMDD：用 Wasserstein 重塑“数据的形”
数据蒸馏的难点不在于“多少”，而在于“像不像”。面对高维、高复杂度的数据分布，要在极少样本下保住内部几何结构，关键是找到能真正刻画“形状”的度量。WMDD（Wasserstein Metric-based Dataset Distillation）的出发点朴素而直接：用最优传输意义下的 Wasserstein 距离做分布对齐，并以其“重心”（barycenter）作为每一类合成数据的几何锚点。

把数据分布想成一团可流动的“质量”，Wasserstein 距离衡量的是把一团搬运成另一团所需的最小“功”。与更偏“统计刻画”的 KL、MMD 不同，它携带几何结构：不是只盯着均值与方差，而是“按形搬运”。对应的 Wasserstein 重心也不是把不同分布简单糊在一起，而是在几何意义上选出能同时贴近各分布的中心形态。我们在论文中用玩具例子对比过这些重心：Wasserstein 能保住圈、叉各自的形，而 KL、MMD 往往趋向“混糊”。这种“保形”的能力，正切中数据蒸馏的要害。

WMDD 的实现顺着这个直觉走到特征空间：先用在真实数据上训练好的分类器，把样本映射到最后一层线性层之前的表征；再在每个类别内计算 Wasserstein 重心。需要强调的是，这里的重心是一组带权代表点，而非一个点，它们以尽量少的“锚”概括类内多样性与几何结构。随后，我们通过梯度下降优化极少量的合成图像，使其特征尽可能贴近相应的重心代表点。为了不让类间统计相互牵扯，WMDD 采用按类统计的 BatchNorm 约束（Per-Class BN, PCBN）：在各层 BN 上对齐每一类自己的均值与方差，既调用了预训练网络的先验，又不破坏类内结构的学习。为保证可扩展性，我们使用高效的最优传输求解，在“位置—权重”上交替优化重心，使整体计算与存储开销与主流分布匹配式蒸馏处于同一量级。

为什么它既跑得动又跑得好？一方面，Wasserstein 的“搬运视角”提供了更有指向性的优化梯度：不是抽象地拉近某个统计量，而是把合成样本朝真实分布的相应区域推近；而 MMD 的效果常受核与带宽设定所限，并不稳定地反映我们关心的几何关系。另一方面，在合理的 Lipschitz 假设下，真实分布与合成分布在期望风险上的差异可以被 W1 距离上界，这给“用 Wasserstein 做对齐”提供了可解释的学习信号。考虑到规模化的现实，WMDD 也探索了 Sliced Wasserstein 作为更快替代，几乎不损精度却略有提速，说明增益来自“几何”，而非某个求解器的技巧。

在 ImageNette、Tiny-ImageNet 与 ImageNet-1K 上，WMDD 在 1/10/50/100 IPC 多种预算下给出强竞争甚至领先结果。以 100 IPC 为例，ImageNet-1K 的 top-1 可达约 60.7%，逼近全量训练的 63.1%。更重要的是跨架构泛化：用 ResNet-18 蒸出的合成数据训练 ResNet-50/101 仍有稳定增益，迁移到 ViT 也能保持可观表现，说明 WMDD 学到的不是“过拟合某个骨干”的捷径，而是贴近真实分布的特征几何。效率上，重心计算的可解性让 WMDD 的时间与显存开销与最稳健的分布匹配方法相当，却在精度上实现了全面超越。

消融显示两项设计相互成就：把特征匹配从交叉熵改为 Wasserstein 重心回归带来稳定提升；把全局 BN 对齐改成 PCBN 后，类内多样性得到更好保持、分布不易塌缩，与重心匹配的目标形成共振。超参也颇为鲁棒：正则过小会出现高频伪影，像是在“迎合”某个权重而非分布本身；在合理区间内，学习始终指向“保形”。

从更宏观的视角看，WMDD 把数据蒸馏拉回“分布几何”的基本图景：用最小搬运代价贴近真实数据的形，既不丢类内多样性，也不破坏类间关系。这种方式天然适配预训练表征，也为与生成式模型的进一步结合铺路。我们也注意到，任何对真实分布的忠实拟合都可能放大数据中的偏置，这需要配合公平性约束与审计；但从能耗节约到算力门槛的降低，其综合社会效益同样不容忽视。

小结与过渡
如果说 WMDD 关心的是“分布的形”，那么接下来这项工作关心的是“损失的形”。当合成数据已经更像真实数据，我们还能否把“抵抗对抗攻击”的能力也一并蒸进去？答案来自对损失几何的再塑形。

GUARD：把“弯曲”抹平，把鲁棒性也蒸出来
直觉上，把对抗训练硬塞进蒸馏流程似乎最直接：用鲁棒教师提炼更抗攻击的合成样本。但实践提醒我们这条路并不稳当——哪怕只在蒸馏阶段用很弱的 PGD，干净精度也会显著崩塌，鲁棒性提升并不稳定。原因在于对抗训练会改变图像的语义表征，而蒸馏又极度压缩分布，两者叠加容易把“精华”扭曲掉。

GUARD（Geometric Regularization for Adversarially Robust Dataset）的核心是一个简洁的理论视角：把样本在小球扰动内的对抗损失近似展开，主导项来自损失对输入的最大曲率（Hessian 最大特征值）。在蒸馏分布已较好贴近真实分布、且最优点附近梯度项相对次要时，只要把损失面的曲率压低，让它在局部更“平”，就能把鲁棒性从蒸馏数据迁移到真实数据。这带来一个重要推论：真实数据与蒸馏数据上的对抗损失上界只相差一个与特征偏差成正比的常数项，因此“在蒸馏数据上优化鲁棒性”是可靠可迁移的。

直接计算最大曲率代价高，GUARD 借助一个经验证据：梯度方向与最大曲率方向在输入空间往往高度相似。于是，我们在单位梯度方向做小步长扰动，最小化两处梯度的差异范数，相当于沿最重要方向把损失面“拉直”——这是对曲率的高效近似。实现上，我们把该正则嵌入主流蒸馏流程（如 SRe2L）的 squeeze 阶段，将其训练损失替换为“原损失 + 曲率正则”，每次迭代只多一次前向与梯度计算，无需昂贵的内层对抗环路，几乎是“即插即用”。

在 ImageNette、Tiny-ImageNet 与 ImageNet-1K 上，GUARD 在多种攻击（PGD、AutoAttack 等）下普遍提升鲁棒指标，并常常“顺带”提高干净精度。例如，ImageNette 的 10 IPC 设定下，干净准确率由标准蒸馏的 42% 提至约 58%，PGD100 从约 5% 提至约 24%；在 Tiny-ImageNet 50 IPC 上，PGD100 从近乎为零的水平显著提升到双位数。随着 IPC 增加，正则强度可相应减弱以避免抑制细粒度判别，整体呈现出“更平的损失几何既稳鲁又利于泛化”的一致规律。

为什么“把弯抹平”能帮助蒸馏鲁棒性？从机制直觉看，这是一种“几何上的容量匹配”。在极稀疏的合成支撑集上学习，若教师的损失曲面在真实邻域里曲率很大、最陡上升方向频繁改变，学生就很难从教师处恢复稳定的一致信号；相反，当我们压低最大曲率，让梯度在小球内不剧烈摆动，合成样本更稳定地贴近类条件流形，学生也就不容易被偶然的高曲率尖峰牵着走，从而同时提升鲁棒与干净泛化。我们也验证了 GUARD 的普适性：将其加到 DC、CDA 等不同蒸馏范式，同样能带来一致收益。

需要强调的是，GUARD 并非形式化的全局鲁棒保证；理论使用了合理但理想化的假设（例如局部凸性、特征映射的 Lipschitz 等）。但大量实证表明，这一几何约束在深度网络的局部区域行之有效：它避免了对抗训练那样的三层优化开销，却以极低的增量代价把“强健的损失几何”注入蒸馏流程，成为一种可推广的鲁棒蒸馏范式。

结语
WMDD 与 GUARD 分别从“分布几何”与“损失几何”两端，给数据蒸馏补齐了“像”与“韧”。前者用 Wasserstein 重心把数据的形状蒸出来，后者用曲率正则把攻击的“弯”抹平；一条保真，一条强健，彼此呼应。展望未来，我们看好两者的深度结合：在联合空间里同时对齐 P(X, Y) 的形与决策边界的平滑性，或将进一步降低算力门槛、提升可信度。论文与代码均已开源，期待社区在更大规模与更多任务上检验、拓展这两条路径，让“小而强”“小而稳”的数据集成为通往可信 AI 的一条可持续之路。
