Dataset Distillation（DD）的两个关键问题：WMDD 与 GUARD

数据集蒸馏（Dataset Distillation，DD）正在成为大模型时代一条务实的“降本增效”路径：用每类极少的合成样本训练出接近全量数据的模型，从而显著降低时间、算力与能耗。真正的挑战在于两点：其一，如何在极少的样本中尽可能保留原始数据分布的几何特性；其二，如何在不显著增加成本的情况下，使在 DD 上训练的模型在对抗扰动下仍保持可靠。本文介绍我们近期的两项工作：ICCV 2025 的 WMDD 针对前者，AAAI 2025 的 GUARD 针对后者。

下文分别介绍两项方法的动机、做法与主要结果。

WMDD：用 Wasserstein 度量保留数据分布的几何特性

数据蒸馏要做的，归根到底是“少而不失真”。WMDD（Wasserstein Metric-based Dataset Distillation）的出发点很直接：既然我们关心分布的几何形态，就用最能度量形状的距离来对齐它——Wasserstein 距离，并以其在几何意义上的“重心”（barycenter）作为蒸馏目标。

直观地说，如果把数据分布想象成一团可以流动的“质量”，Wasserstein 衡量的是把一团搬运成另一团的最小“功”；它带着几何去对齐每一份质量，而不是只盯住均值或协方差。相应的 Wasserstein 重心，也不是把若干分布简单“糊”在一起，而是找到能同时接近它们的中心形态——在形状上保留各自结构特征。这种“保形”的能力，正是数据蒸馏最需要的。

WMDD 把合成数据的学习搬到特征空间中：先用在原始数据上训练好的分类器，把每类样本映射到最后一层线性分类器之前的特征空间，再在该空间分别计算各类的 Wasserstein 重心。这里的重心不是一个点，而是一组带权代表点，用尽量少的“锚”概括类内多样性与几何结构。随后，我们用梯度下降优化每类少量合成图像，使其特征贴近对应的重心代表点。同时引入一个简单但关键的正则：按类统计的 BatchNorm 约束（Per-Class BN，PCBN）。不同于以往全局 BN 对齐容易让类别间的梯度相互牵扯，PCBN 在类内对齐各层 BN 的均值与方差，既利用了预训练网络的先验，又不打乱类内结构的学习。为保证可扩展性，我们采用高效的最优传输求解，对重心的“位置—权重”交替优化，计算与存储开销与主流分布匹配式蒸馏方法处于同一量级。

为什么这套方案能在高分辨率数据集上既可算、又好用？一方面，Wasserstein 提供的“搬运”视角让优化梯度更有指向性：不是拉近某个统计量，而是把合成样本明确推向真实数据的相应区域；另一方面，在合理的 Lipschitz 假设下，真实分布与合成分布在期望风险上的差异可以被 W1（Wasserstein-1）距离上界，这为“对齐形状就能减少偏差”提供了直观解释。相比许多为规模化而仅对齐特征均值的 MMD 实践，Wasserstein 重心更敏感于结构差异，同时借助特征空间与高效 OT 求解保持了可计算性。

实验上，WMDD 在 ImageNette、Tiny-ImageNet 与 ImageNet-1K 上，在 1/10/50/100 IPC 的多种预算下都给出强竞争甚至领先的结果。以 100 IPC 为例，三套数据的 top-1 分别约 87.1%、61.0% 与 60.7%，逼近全量数据训练的同架构模型（约 89.9%、63.5%、63.1%）。跨架构泛化同样稳健：用 ResNet-18 蒸馏得到的合成数据迁移到 ResNet-50/101 或小型 ViT 仍有可观增益。这说明 WMDD 学到的不是“过拟合某个骨干”的捷径，而是真正在特征几何上贴近了真实分布。得益于重心计算的可解性，时间与显存开销与当前最为高效的方法处于同一数量级。

消融分析也支持上述选择：把特征匹配从交叉熵替换为 Wasserstein 重心回归带来稳定增益；把全局 BN 正则改为 PCBN 后，类内多样性更好、特征分布不再“塌缩”；用 Sliced Wasserstein 作为更快替代几乎不损精度且略有提速。总体上，WMDD 的核心收益确实来自 Wasserstein 几何，而非某个工程技巧或娇气的超参。

从更宏观的视角看，WMDD 把数据蒸馏拉回“分布几何”的基本图景：用最小搬运代价贴近真实数据的形，既不丢掉类内的多样性，也不破坏类间的相对关系。这种方式天然适合与预训练表征协同，也为与生成式模型的结合打下基础。我们也在思考两个自然的延伸：其一，直接在联合空间蒸馏 P(X, Y)，将标签嵌入表示空间并在联合度量下对齐，有望同时捕捉标签边际分布与标签间关系结构（例如标签嵌入的相似性），从而改进跨类迁移与长尾类别的刻画；其二，“最优”的合成数据未必就是全局最接近原始分布的那一组，决策边界附近的代表性样本可能更关键，因而一种折中是让部分样本的位置介于“重心代表点”和“靠近判别边界的支持点”之间，把最优传输的全局对齐与边界敏感的判别信号结合起来。

在“保形”之外，可靠性同样关键：当模型面对对抗扰动时，训练在蒸馏数据上的模型还能扛得住吗？这正是我们的第二项工作所聚焦的方向。

GUARD：在蒸馏过程中平滑损失景观以获得对抗鲁棒性

最直接的想法，是把对抗训练嵌入蒸馏流程；但实践显示，这会显著拉低干净集性能，鲁棒收益也并不稳定。我们因此转向更本质的做法：不在外层堆叠昂贵的对抗环路，而是从损失景观出发，直接在真实数据邻域平滑模型的局部曲率。这就是 GUARD（Geometric Regularization for Adversarially Robust Dataset）的核心。

理论上，把样本在小球扰动内的对抗损失近似展开，可以得到一个上界，其中主导项来自损失对输入的最大曲率（Hessian 最大特征值）。当蒸馏数据与真实分布在特征空间足够接近时，真实数据与蒸馏数据上的对抗损失上界只相差一个与偏差 σ 成正比的常数项——因此，“在蒸馏数据上优化鲁棒性”能够可靠地迁移到“在真实数据上评测鲁棒性”。

实现上，我们用一个高效近似去最小化最大曲率：借助经验事实——梯度方向与最大曲率方向在输入空间往往高度相似——在单位梯度方向做小步长扰动，最小化两处梯度的差异范数，使损失景观在该方向上更接近局部线性。我们把该正则嵌入 SRe2L 的 squeeze 阶段，将标准训练损失替换为“原损失 + 曲率正则”，每步仅多一次前向与梯度计算，无需内层对抗环路，代价极低。

在 ImageNette、Tiny-ImageNet 与 ImageNet-1K 上，覆盖 10/50/100 images per class（IPC）的多种设置，GUARD 在多种白盒/黑盒攻击（如 PGD100、AutoAttack 等）下普遍提升鲁棒指标，并常常“顺带”提升干净精度。例如，在 ImageNette 10 IPC 上，干净准确率从 42.42% 提至 57.93%，AutoAttack 从 4.99% 提至 19.69%；在 Tiny-ImageNet 50 IPC 中，PGD100 从 0.27% 提升到 15.63%，AutoAttack 从 0.16% 提升到 13.84%；当压缩比例放宽到 ImageNette 100 IPC，PGD100 由 31.65% 提升到 57.50%，AutoAttack 由 17.93% 提升到 64.84%。在更具挑战的 ImageNet-1K 10 IPC 上，干净准确率与鲁棒性也都获得显著提升。更重要的是，GUARD 作为一种“损失景观正则”的做法，具有良好的普适性：将其加入 DC、CDA 等不同蒸馏范式，同样能同时提升干净与鲁棒性能。

背后的直觉并不复杂：在极稀疏的合成支撑集上学习时，如果教师模型在真实数据邻域的损失景观崎岖、曲率大，那么最陡上升方向会频繁改变，蒸馏过程就难以从教师处稳定地恢复可对齐的梯度场；相反，降低最大曲率、让梯度在小球内不剧烈摆动，能减少对抗扰动最敏感方向的“尖锐性”，学生模型更易贴近类条件流形，从而同时提升鲁棒性与干净泛化。经验上，这种基于曲率的“容量匹配”在较小 IPC 下收益更显著；当 IPC 较大时，适度减弱正则强度可避免抑制必要的细粒度判别。

需要说明的是，我们并不把它当作形式化的全局鲁棒保证；若干假设（如局部凸性与特征映射的 Lipschitz）是合理但理想化的。然而，它以极低的额外开销，在大规模数据与强攻击设置下给出了兼顾精度与鲁棒的均衡解。论文与代码已开源，欢迎检阅与复现：https://github.com/yumozi/GUARD。

收束与展望

这两项工作分别回答了 DD 的两个关键问题：如何在极低样本预算下尽可能保留原始数据分布的重要几何特性，以及如何在不显著增加训练成本的情况下获得可迁移的对抗鲁棒性。更凝练地说，通过让神经网络在蒸馏数据上学到更稳健的表示，并在表示空间对数据分布的重要方面做更精准的匹配，DD 有望在准确性与稳健性之间取得兼顾。向前看，值得探索的方向包括：在联合空间直接蒸馏 P(X, Y)，在复杂场景中将生成建模与重心表达结合以提升类内多样性，对曲率正则进行更细粒度的自适应以适配不同的 IPC，以及将这些思路推广到分布外与多模态设置中。我们希望“小而强”的数据集能成为走向可信与高效 AI 的一条可持续道路。
